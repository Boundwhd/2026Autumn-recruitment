### Float4 向量化访存到底带来了什么？
1. 对于访存单元而言，需要发射的访存指令更少了。假设用的是正常的float，那么读取4个float的话，需要发射4条LD.E指令。而使用float4的话，只需要发射一条LD.E.128指令。那直观上性能自然更优。但需要说明的是，在Nvidia的GPU中，由于SIMT架构是通过切换warp来掩盖访存的延时，所以并不代表着4个cycle发射4条指令就比1个cycle发射1条指令慢4倍，大部分的时间其实都是在等待访存单元把数据拿回来，而真正访存的时间，不管是去L1还是L2拿数，cache line都是128Byte。float和float4都是一样。对于一个warp而言，如果32个线程想要去拿128个数，不管float还是float4，都得变成4次对cache line的读取，当然，如果仅仅是拷贝，这个cache line还不一定命中。如果到了global mem中去读，从硬件的角度而言，访存端口也是一样，并不会因为float4就能够获得更多的端口读数。所以结论是，float4会更快，但是快得不多。

2. 对于指令cache而言，所需要的指令更少了，那么icache不命中的概率就会减少很多。我们假设一个场景，对于一段核心代码，volta架构有12KB的L0 指令cache，一条指令需要128bit，那么最多可以容纳768条SASS指令，对于sgemm中的核心循环，假设取12行12列，组成12x12=144条FFMA指令，循环展开6次，144x6=576条指令，一共有(12+12)*6 = 144个数需要load，如果用float则需要144条指令，那么计算和访存一共有720条指令，再加一些其他的指令，很容易导致指令cache放不下，性能有所损失。如果用float4的话，则需要144/4 = 36条指令，总共612条指令，指令cache肯定能放得下。

当然，转成float4也会产生一些负面的影响，首先是所采用的寄存器更多了，寄存器资源被占用多了之后，SM中能够并发的warp数量会有所减少。此外，如果本身程序的并行粒度就不太够，使用float4的话，所使用的block数量减少，warp数量减少，性能也会有一定的影响。所以如果是并行粒度本身不太够的情况下，还是需要谨慎地考虑是否采用float4这样的向量化数据。


### SIMT和SIMD的区别
#### 从程序员的角度来看，SIMT和SIMD在编程模型和代码优化上也存在一些差异:

1. SIMT采用的是线程模型,程序员主要针对线程块(block)和线程(thread)的组织来优化,利用更多线程来隐藏延迟,提高并行粒度。SIMD采用数据并行模型,程序员需要通过向量化、循环展开等方式提高数据级别的并行性。
2. SIMT线程间可以使用 barrier 同步或者 atomic 操作通信,而SIMD通常需要程序员明确的使用缓存对齐,避免向量化时的内存数据冲突。
3. SIMT编程可以不需要考虑硬件特性,由编译器自动优化生成设备代码。而SIMD需要充分考虑目标硬件的特点,比如向量寄存器长度,最大带宽等。
4. SIMT更适合不规则,动态的代码,程序员主要负责算法和组织并行框架。SIMD对代码结构要求更高,程序员需要关注每一条指令是否可以向量化。
5. SIMT对程序员的要求较低,学习曲线更平稳。而SIMD编程门槛更高,需要较强的并行编程能力。
6. SIMT可移植性更好,同一代码可以在不同GPU架构上运行。而SIMD优化往往跟特定指令集严密相关。
7. SIMT调试更简单,可以在CUDA中设置断点,打印日志。而SIMD需要更底层的调试。

#### 从指令集的角度来看,SIMT和SIMD主要有以下几点区别:

1. 指令并行粒度不同。SIMD的指令通常并行度较小,如128bit或256bit,一次执行几个单精度浮点运算。而SIMT可组合更多线程,一次同步执行数百条指令。
2. 寻址模式不同。SIMD使用连续的向量寻址,同一指令操作一个向量/矩阵。SIMT允许各线程独立寻址,访问不同地址空间。
3. 流水线不同。SIMD采用传统的重叠流水线,提高单指令吞吐。SIMT使用更宽的流水线,同时执行多指令,需要更复杂的调度和同步。
4. 指令集特性不同。SIMD提供丰富的向量计算/转换指令。SIMT提供线程同步,通信原语。
5. 控制流处理不同。SIMD中的所有执行单元必须严格同步。SIMT支持线程级控制流,可执行不同路径。
6. 编译实现不同。SIMD由标准编译器优化。SIMT需要专门的设备编译器,处理复杂的线程调度和变量映射。
7. 硬件实现不同。SIMD扩展现有指令集和流水线。SIMT需要专门的多处理器和调度器。



### LlamaInference
#### 内存管理优化

##### 旧版本
首先，申请到的内存被封装在 buffer 类中，buffer 类存放了一个内存使用的一些成员：
1. 内存地址
2. 内存大小
3. 是否处于使用状态

在 CUDA 内存管理类中存放两个内存池，分别存放大小 大于1MB 和 小于1MB 的内存 buffer，内存池采用的是 vector 容器。申请到的内存根据实际大小进入相应的内存池中，将忙碌成员置于 True。
当有新的内存申请需求时，根据申请的内存大小去相应的内存池中寻找比申请内存大的且处于不忙碌的 buffer，（此处一定是O(n)时间复杂度，因为需要找到比他大的且差距最小的 buffer）。若差距最小的 buffer 超过一定了阈值，或者找不到这样一块合适的内存，则重新申请内存（避免内存的浪费）。若无法申请内存（out of memory），则一次性释放内存池内所有忙碌状态为 False 的内存buffer。若找到了这样一块合适的 buffer，则将该 buffer 忙碌状态置为 True，返回这个 buffer。
释放内存时，将该 buffer 块的忙碌成员置于 False。

##### 问题：
1. 内存池的组织和查找效率：每次分配时需要在 vector 中进行线性查找（O(n)），以找到最合适（最小且足够大）的 buffer。对于频繁的内存分配/释放操作，O(n) 的查找时间可能会成为性能瓶颈，尤其是在内存池中 buffer 数量较多时。
2. ​碎片化：由于总是选择最接近申请大小的 buffer，可能会导致内存池中留下许多小的、难以利用的碎片。
3. 阈值：阈值的设置需要合理，如果阈值太小，可能会导致频繁重新申请内存，增加开销；如果阈值太大，可能会导致内存浪费。阈值的动态调整可能更合理，而不是固定值。
4. vector 的使用问题：vector 的内存是连续的，如果频繁插入/删除 buffer，可能会导致内存重新分配和数据拷贝。
5. 释放策略的优化：可以考虑定期或根据内存压力释放部分非忙碌 buffer 回系统，避免内存占用过高。
6. 内存池的统计和监控：内存池缺乏对内存使用情况的统计（如总内存、空闲内存、碎片情况等）。


##### 新版本
1. buffer 类的组织结构变化：buffer 类以双向链表的结构组织，在O(1)的时间复杂度内查找与自己相邻的 buffer块，用于后续内存合并。
2. 申请策略：每次申请时申请大于实际需求的空间，具体为：
    - 小于 1MB 的申请，实际申请 2MB
    - 1MB - 10MB 的申请，实际申请 20MB
    - 为大于等于10MB 的申请，实际分配向上取整到2MB的整数倍

    这样的好处在于：
    - 减少频繁申请：预分配大块内存，减少 cudaMalloc 调用
    - 采用预分配的方法，避免多次申请的性能开销，
    - 动态调整：避免固定大小分配导致的内存浪费
3. 内存池更新：内存池的结构由 vector 容器变为 set。不忙碌的 buffer 块将根据其实际大小进入对应的内存池中。首先按照内存大小的顺序排列，其次按照地址大小排列。这样的好处在于
- 查找块的时间复杂度变为O(logN)，加快查找的效率。
- 删除块，添加块的性能比起 vector 

4. 申请块的步骤：在相应的内存池中找对应的 buffer，若找到后，计算使用后浪费的碎片大小，若浪费的碎片大小大于阈值，则 split 该浪费的碎片重新进入合适的内存池，并且于前面的内存块组织成双向链表（split切分，地址连续）。若无法找到合适的 buffer，重新申请内存。

5. 释放策略：统计信息统计总内存，已申请内存，忙碌中内存，空闲内存。释放时首先通过 unorder_map 找到该内存对应的 buffer 块，将 buffer 置于非忙碌状态，并且通过双向链表与前后的空闲 buffer 块进行内存合并。统计空闲内存大小，若空闲内存大小超过一定阈值，free 空闲内存。这样的好处在于避免长期不释放空闲内存会导致 GPU 内存占用持续增长，最终可能触发 OOM（Out of Memory）。减少 cudafree 的调用开销。

6. 阈值的选取：通过实际工程中的实际数据，进行统计计算来决定。
- 目标：统计 Split 后的碎片大小分布
- 监控 free_memory 和 OOM 频率
- 分析历史请求分布

