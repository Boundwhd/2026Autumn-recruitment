## 矩阵乘法的CUDA优化

### 实验设置
1. 矩阵大小配置为：$A[4096, 4096] * B[4096, 4096] = C[4096, 4096]$
2. 浮点运算次数：$ \text{flops} = 2 \times 4096 \times 4096 \times 4096 \approx 128\text{G flops}$

### Kernel 1. 基础实现
一个线程负责一个像素点的计算。例如线程（0,0）和线程（1,0）他们会访问 B 矩阵的同一列（第0列），但是访问 A 矩阵的不同行（第1行与第2行）。如果不考虑任何缓存，则每个线程要从 global memory 加载 2 * 4096 floats 的数据。我们设置了 4096 * 4096 个线程，这会导致数据的搬运达到惊人的 512G。其次，一个 Warp 内的线程并没有访问连续的数据，导致无法达到带宽的峰值利用率。

### Kernel 1++. 全局内存合并
一个 Warp 通常包含32个线程，被分配给调度器，执行相应的指令。同一 Warp 内的线程对全局内存的连续访问可以被合并成一次执行，可以实现峰值带宽的重要因素（若访问数据不连续，则会增加内存请求次数，浪费带宽，合并访问后可以逼近硬件的峰值带宽）。实际上，GPU 支持 32B，64B，128B的内存访问，Warp 调度器可以把32个线程的连续内存请求合并成单次内存事务。

修改矩阵 A 和 B 的访问模式：B矩阵采用全局内存合并，一个 Warp 内的线程可以访问连续的32个float数据，合并单次内存申请事务，最大化带宽的利用。此时，A矩阵将有多个线程同时访问 global memory 中同一地址，是否发生优化取决于硬件架构。

### Kernel 2. 共享内存-分块缓存
GPU 上有一个片上的存储区域，称为共享内存，每一个 SM 上都有一个共享内存（shared memory），同一个 block 上的线程可以通过共享内存与其他线程通信。在我的 RTX 3090上，每个 block 都有一个最大 48KB 的共享内存。我们在这个 kernel 的实现中将矩阵分块，每一个 block 处理一个块上的像素。设置 block 中的线程数量等于块中的元素数量，用一个线程去计算 block 中的一个矩阵元素。

通过查看性能分析，我们发现该 kernel 的 Stall MIO Throttle 较高，这个指标意味着在 MIO 流水线（包括特殊数学指令、动态分支和共享内存指令）的利用率极高。我们并没有使用特殊数学指令以及动态分支，所以初步分析我们是因为等待 shared memory 结果返回而等待。所以我们通过​让每个线程处理多个输出元素，减少对共享内存（SMEM）的依赖，转而利用​寄存器（Registers）进行更多计算，从而优化性能。

### Kernel 3. 一维块平铺-每个线程计算多个结果
由于上一个 kernel 依赖于 smem 的数据获取，线程大部分时间都在等待数据的搬运，我们这次将一个线程计算多个像素的结果，让线程充分忙碌起来。
假设一个线程处理 TM 个像素的计算，为了保证访存的连续性以及 smem 的广播机制，我们尽量保证没有 bank conflict。
<img src="/home/whd/2026Autumn-recruitment/HPC/cuda/gemm/image/kernel4.png" width="100%">
注意连续内存的访问，以及 smem 广播机制的使用，合理控制块大小确保性能的最优。一个线程计算同一列不同行的像素，保证多个线程在写入矩阵时写入地址的连续性。

### Kernel 4. 二维块平铺-每个线程计算多个结果
每个线程开辟三个寄存器，第一个存放 A 中的 TM 个元素，第二个存放 B 中的 TN 个元素，结果寄存器中存放 A 和 B 中元素交错相乘的所有结果（结果寄存器）。
<img src="/home/whd/2026Autumn-recruitment/HPC/cuda/gemm/image/kernel5.png" width="100%">
但是，在 A 矩阵读取数据时会出现严重的 bank confict。B 矩阵出现少量 bank conflict