## vllm
vLLM是一个开源的大模型推理加速框架，通过PagedAttention高效地管理attention中缓存的张量。
快Transfomer推理速度24倍。

### KVcache浪费
大模型推理的性能瓶颈主要来自于内存
- 自回归过程中缓存的K和V张量非常大，在LLaMA-13B中，单个序列输入进来需要占用1.7GB内存。
- 二是内存占用是动态的，取决于输入序列的长度。由于碎片化和过度预留，现有的系统浪费了60%-80%的内存。

1. 预分配，但不会用到
2. 预分配，但尚未用到
3. 显存之间的间隔碎片，不足以预分配给下一个文本生成。

### PagedAttention

PagedAttention灵感来自于操作系统中虚拟内存和分页的经典思想，它可以允许在非连续空间里存储连续的KV张量。
将kvcache的利用率从20%-40%提高到90%

    **优势**：
    1. 按需分配，不提前预分配
    2. 按block分配，减少碎片大小
    3. 虚拟内存，方便实现调用

    - 将n个token分为一个block，连续分配
    - 当一个block占满，会分配一个新的block，空间为n个token
    - 内存碎片最多为n-1个token的大小

        1. 虚拟内存：每个请求都会有逻辑的KVcache，在逻辑的KVcache中，显存似乎是连续的。
        2. 映射表：vllm框架维护一个虚拟KVcache到物理显存KVCache的映射表。
        3. 在进行计算时，会通过映射表找到物理显存地址上的KVcache进行计算。

### Sharing KV Blocks
用大语言模型同一个Prompt，希望生成多个Output时：

1. 提示词在显存中只会生成一分，但会标记多个引用
2. 当开始生成第一个序列的新词时，触发 copy on write 机制