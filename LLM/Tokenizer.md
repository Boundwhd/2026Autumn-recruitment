# 分词器

分词是自然语言处理的基础，分词准确度直接决定了后面的词性标注、句法分析、词向量以及文本分析的质量。英文语句使用空格将单词进行分隔，除了某些特定词，如how many，New York等外，大部分情况下不需要考虑分词问题。但中文不同，天然缺少分隔符，需要读者自行分词和断句。故在做中文自然语言处理时，需要先进行分词。

## 中文分词算法
- 基于词典匹配

对于中文分词问题，最简单的算法就是基于词典直接进行greedy匹配。比如，我们可以直接从句子开头的第一个字开始查字典，找出字典中以该字开头的最长的单词，然后就得到了第一个切分好的词。

- 基于统计
    - 基于语言模型
    
    在各种切词组合中找出那个最合理的组合，这个过程就可以看作在切分词图中找出一条概率最大的路径。

    这个可以给词序列存在合理性打分的东西就叫做“语言模型”。
    - 基于统计机器学习

    中文分词同样可以建模成一个“序列标注”问题，即一个考虑上下文的字分类问题。因此可以先通过带标签的分词语料来训练一个序列标注模型，再用这个模型对无标签的语料进行分词。

- 基于神经网络
    - 3.1 基于(Bi-)LSTM
    - 基于预训练模型+知识蒸馏


# 中文分词器：jieba
算法
- 基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG)
- 采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合
- 对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法


## GOOGLE: 
- word piece
- sentence piece

## Open-AI：
- Ticktoken: 

GPT2 的分词器是基于BPE算法，但有改造。利用了一些正则表达式，把句子先拆成希望的单词的组合，包括特殊符号、数字、等等组合方式，再使用BPE算法。具体分词器训练代码未开源。

GPT2 的空格分词并未合并，但GPT4的空格合并了



